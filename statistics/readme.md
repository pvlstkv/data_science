# Confidence interval (CI)
## Definition
A confidence interval is a range that covers the true value of a parameter with a certain level of confidence. Literally, this means that if we measure a confidence interval with a confidence level of, for example, 95% many times, then we will get that 95% of our intervals on average cover the true value.
The most popular value that is measured using CI is a mean

$$mean \pm t_{cl, df} * {std \over \sqrt{n}}$$

where mean is average of the sample,
std is unbiased stadard deviation,
n - size of sample
$$t_{cl, df}$$ Studets value of confidence level = cl, and degress of freedom = df = n - 1
